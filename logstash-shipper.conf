# Where to get input
input {
  # syslog inputs
  tcp {
    port => 5000
    type => "syslog"
  }
  udp {
    port => 5000
    type => "syslog"
  }

  # Lumberjack input
  lumberjack {
    port => 5002
    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
    ssl_key => "/etc/logstash/ssl/logstash-forwarder.key"
    type => "lumberjack"
  }

  # CoreOS journal input
  tcp {
    codec => "json_lines"
    port => 5004
    tags => ["coreos","docker"]
    type => "systemd"
  }

  # Logspout input
  tcp {
    codec => "json_lines"
    port => 5006
    tags => ["docker"]
    type => "logspout"
  }

  # Log4j application input
  log4j {
    codec => "json_lines"
    port  => 4560
    tags  => ["applogs"]
    type  => "log4j"
  }
  log4j {
    codec => "json_lines"
    port  => 5200
    tags  => ["applogs","service"]
    type  => "log4j"
  }
}

# Some Filtering
filter {
  # syslog/systemd filter
  if [type] == "syslog" or [type] == "systemd" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{SYSLOGPROG}: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
    }
    syslog_pri { }
    date { match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ] }

    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => { "message" => "%{syslog_message}" }
        remove_field => [  "syslog_message", "syslog_program" ]
      }
    }

    # Remove spurious fields that have names changed or been aggregated
    mutate { remove_field => [ "syslog_hostname", "syslog_timestamp" ] }
  }

  # Docker filter
  if [program] == "dockerd" {
    kv {
      source => "message"
      prefix => "docker_"
    }
    mutate {
      rename => { "docker_level" => "docker_loglevel" }
      replace => { "message" => "%{docker_msg}" }
    }
    mutate { remove_field => [ "docker_msg", "docker_time" ] }
#    mutate {
#      replace => { "message" => "time=\"%{TIMESTAMP_ISO8601:docker_ts}\" level=%{LOGLEVEL:docker_loglevel} msg=\"%{GREEDYDATA:docker_msg}\"" }
#    }
  }

  # Logspout filter
  if [type] == "logspout" {
    grok {
      match => { "message" => "%{SYSLOG5424PRI}%{NONNEGINT:ver} +(?:%{TIMESTAMP_ISO8601:ts}|-) +(?:%{HOSTNAME:containerid}|-) +(?:%{NOTSPACE:containername}|-) +(?:%{NOTSPACE:proc}|-) +(?:%{WORD:msgid}|-) +(?:%{SYSLOG5424SD:sd}|-|) +%{GREEDYDATA:msg}" }
    }
    syslog_pri { }
    date { match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ] }
    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => { "@source_host" => "%{syslog_hostname}" }
        replace => { "message" => "%{syslog_message}" }
      }
    }

    # Remove spurious fields that have names changed or been aggregated
    mutate { remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ] }
  }

  # Add GeoIP
  geoip { source => "%{IPORHOST}" }
}

# Where to send output
output {
  # Redis queue output
  redis {
    data_type => "list"
  # host => ["localhost:6379"]
    host => ["REDIS_CONN_STR"]
    key => "logstash"
    codec => "json"
  }

  # Kafka queue output
#  kafka {
   # broker_list => ["localhost:9092"]
#    broker_list => ["KAFKA_BROKERS"]
#    codec => "json"
#    compression_codec => "snappy"
#    topic_id => "logstash"
   # host => ["localhost:6379"]
#    host => ["KAFKA_CONN_STR"]
#  }
}
